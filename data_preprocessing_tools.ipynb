{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_preprocessing_tools.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNxDRfLvKVBN9HjXcmlURF3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"37puETfgRzzg","colab_type":"text"},"source":["# Data Preprocessing Tools"]},{"cell_type":"markdown","metadata":{"id":"EoRP98MpR-qj","colab_type":"text"},"source":["## Importing the libraries"]},{"cell_type":"code","metadata":{"id":"N-qiINBQSK2g","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RopL7tUZSQkT","colab_type":"text"},"source":["## Importing the dataset"]},{"cell_type":"code","metadata":{"id":"WwEPNDWySTKm","colab_type":"code","colab":{}},"source":["dataset = pd.read_csv('Data.csv')\n","X = dataset.iloc[:, :-1].values # The features (the independent variables) => Takes all the rows and all the columns except the last column\n","y = dataset.iloc[:, -1].values # The dependent variable (that we want to predict) => Takes all the rows and the last column (dependent variable vector)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"hCsz2yCebe1R","colab_type":"code","outputId":"1e4cc568-4e51-4b38-9d46-4aa3f15204be","executionInfo":{"status":"ok","timestamp":1587622253093,"user_tz":-240,"elapsed":895,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"colab":{"base_uri":"https://localhost:8080/","height":188}},"source":["print(X)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[['France' 44.0 72000.0]\n ['Spain' 27.0 48000.0]\n ['Germany' 30.0 54000.0]\n ['Spain' 38.0 61000.0]\n ['Germany' 40.0 nan]\n ['France' 35.0 58000.0]\n ['Spain' nan 52000.0]\n ['France' 48.0 79000.0]\n ['Germany' 50.0 83000.0]\n ['France' 37.0 67000.0]]\n"]}]},{"cell_type":"code","metadata":{"id":"eYrOQ43XcJR3","colab_type":"code","outputId":"e0873b2a-3b08-4bab-ef0d-15b88858ca44","executionInfo":{"status":"ok","timestamp":1587622256072,"user_tz":-240,"elapsed":656,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(y)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"]}]},{"cell_type":"markdown","metadata":{"id":"nhfKXNxlSabC","colab_type":"text"},"source":["## Taking care of missing data"]},{"cell_type":"code","metadata":{"id":"c93k7ipkSexq","colab_type":"code","colab":{}},"source":["from sklearn.impute import SimpleImputer\n","\n","imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n","imputer.fit(X[:, 1:3]) # Computes the mean values for the specified dataframe. Get all rows and the columns from 1 (included) to 3 (excluded)\n","X[:, 1:3] = imputer.transform(X[:, 1:3]) # Actually does the replacement of the missing numerical values and returns a new dataframe. Should be use to replace the part of the dataframe in question"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"3UgLdMS_bjq_","colab_type":"code","outputId":"254af4e0-681e-47f5-aaa7-b9c6f43258e9","executionInfo":{"status":"ok","timestamp":1587622284427,"user_tz":-240,"elapsed":919,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"colab":{"base_uri":"https://localhost:8080/","height":188}},"source":["print(X)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[['France' 44.0 72000.0]\n ['Spain' 27.0 48000.0]\n ['Germany' 30.0 54000.0]\n ['Spain' 38.0 61000.0]\n ['Germany' 40.0 63777.77777777778]\n ['France' 35.0 58000.0]\n ['Spain' 38.77777777777778 52000.0]\n ['France' 48.0 79000.0]\n ['Germany' 50.0 83000.0]\n ['France' 37.0 67000.0]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"CriG6VzVSjcK","colab_type":"text"},"source":["## Encoding categorical data"]},{"cell_type":"markdown","metadata":{"id":"AhSpdQWeSsFh","colab_type":"text"},"source":["### Encoding the Independent Variable"]},{"cell_type":"code","metadata":{"id":"5hwuVddlSwVi","colab_type":"code","colab":{}},"source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# We want to transform the country column (the independent variable with the country names) into some numerical values (because we cannot just feed Strings to the machine learning model later on). We don't want to give them just numbers because that may confuse the model. It can think that there is some kind of numerical relationship between these numbers and the dependent variable. Therefore, we encode the country column using OneHotEncoder, which gives each String a certain binary combination. We have 3 categories in this column: France, Germany, and Spain. So, using the OneHotEncoder, they will be given encoded values like 000, 100, and 010 (just an example). To apply this transformation, we use the class ColumnTransformer\n","ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough') # We want to do an encoding transformation, we pass a OneHotEncoder object, and we want to apply the encoder on the first column (of countries) which have the index 0. For the rest of the columns, we want to do nothing (to leave them intact), that's why the remainder takes the value 'passthrough'\n","X = np.array(ct.fit_transform(X)) # Fit and transform happen in the same step in the ColumnTransformer class"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7QspewyeBfx","colab_type":"code","outputId":"5b35feef-7fe2-46ef-ce70-80495f94f4ed","executionInfo":{"status":"ok","timestamp":1587622291650,"user_tz":-240,"elapsed":570,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"colab":{"base_uri":"https://localhost:8080/","height":188}},"source":["print(X)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.0 0.0 0.0 44.0 72000.0]\n [0.0 0.0 1.0 27.0 48000.0]\n [0.0 1.0 0.0 30.0 54000.0]\n [0.0 0.0 1.0 38.0 61000.0]\n [0.0 1.0 0.0 40.0 63777.77777777778]\n [1.0 0.0 0.0 35.0 58000.0]\n [0.0 0.0 1.0 38.77777777777778 52000.0]\n [1.0 0.0 0.0 48.0 79000.0]\n [0.0 1.0 0.0 50.0 83000.0]\n [1.0 0.0 0.0 37.0 67000.0]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"DXh8oVSITIc6","colab_type":"text"},"source":["### Encoding the Dependent Variable"]},{"cell_type":"code","metadata":{"id":"XgHCShVyTOYY","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# The dependent variable (y) is also text (yes/no), we need also to encode that numerically before feeding the dataset to the model. Here, we have only two categories yes/no, so we can just transform them into 1 and 0. For that, we can simply use the LabelEncoder class (which also has a fit_transform method).\n","le = LabelEncoder()\n","y = le.fit_transform(y)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"FyhY8-gPpFCa","colab_type":"code","outputId":"7f76ef29-5423-4c3e-cf69-45fbc366a997","executionInfo":{"status":"ok","timestamp":1587622297024,"user_tz":-240,"elapsed":657,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(y)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 0 0 1 1 0 1 0 1]\n"]}]},{"cell_type":"markdown","metadata":{"id":"qb_vcgm3qZKW","colab_type":"text"},"source":["## Splitting the dataset into the Training set and Test set"]},{"source":["#### We have to split the dataset into training set and testing set *before* feature scaling. The reason is feature scaling  applies the mean and the standard deviation in the dataset. The test set must be brand new everytime. Applying feature scaling before will leak testing set data into the training set data, and we should avoid that."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"pXgA6CzlqbCl","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1) # test_size = 0.2 means 20% of the dataset will be used for testing (80% will be used for training). random_state = 1 is just for educational purposes: it fixes the randomization seed to 1 so that we always get the same training and test sets (just for education)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"GuwQhFdKrYTM","colab_type":"code","outputId":"de1e527f-c229-4daf-e7c5-ea9d2485148d","executionInfo":{"status":"ok","timestamp":1587622301522,"user_tz":-240,"elapsed":597,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"colab":{"base_uri":"https://localhost:8080/","height":154}},"source":["print(X_train)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0 0.0 1.0 38.77777777777778 52000.0]\n [0.0 1.0 0.0 40.0 63777.77777777778]\n [1.0 0.0 0.0 44.0 72000.0]\n [0.0 0.0 1.0 38.0 61000.0]\n [0.0 0.0 1.0 27.0 48000.0]\n [1.0 0.0 0.0 48.0 79000.0]\n [0.0 1.0 0.0 50.0 83000.0]\n [1.0 0.0 0.0 35.0 58000.0]]\n"]}]},{"cell_type":"code","metadata":{"id":"TUrX_Tvcrbi4","colab_type":"code","outputId":"9a041a9b-2642-4828-fa2f-a431d7d77631","executionInfo":{"status":"ok","timestamp":1587622305066,"user_tz":-240,"elapsed":835,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(X_test)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0 1.0 0.0 30.0 54000.0]\n [1.0 0.0 0.0 37.0 67000.0]]\n"]}]},{"cell_type":"code","metadata":{"id":"pSMHiIsWreQY","colab_type":"code","outputId":"5afe91e0-9244-4bf5-ec1b-e3e092b85c08","executionInfo":{"status":"ok","timestamp":1587622306938,"user_tz":-240,"elapsed":536,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(y_train)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 0 0 1 1 0 1]\n"]}]},{"cell_type":"code","metadata":{"id":"I_tW7H56rgtW","colab_type":"code","outputId":"2a93f141-2a99-4a69-eec5-c82a3bb8d36b","executionInfo":{"status":"ok","timestamp":1587622309210,"user_tz":-240,"elapsed":828,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(y_test)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1]\n"]}]},{"cell_type":"markdown","metadata":{"id":"TpGqbS4TqkIR","colab_type":"text"},"source":["## Feature Scaling"]},{"cell_type":"code","metadata":{"id":"AxjSUXFQqo-3","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import StandardScaler\n","\n","# We are going to do Feature Scaling using Standardization (because it works all the time, while Normalization works only if the feature's values are normally distributed. That's why we are going to use the StandardScaler class)\n","sc = StandardScaler()\n","X_train[:, 3:] = sc.fit_transform(X_train[:, 3:]) # Fit calculates the mean and the standard deviation of each feature, and transform will apply them on the dataset. We don't want to apply the Feature Scaling (Standardization) to the dummy value generated for the country, which after being encoded were transformed into 3 columns with values of 0's and 1's. We don't need to do any changes in these values, therefore, we apply the Standardization from the fourth column on\n","\n","X_test[:, 3:] = sc.transform(X_test[:, 3:]) # The same scaler is applied on the test set, but only the transform method is used because it must be fitted to the training test features (This is very important)\n"],"execution_count":17,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-17-1ff7646f9e93>, line 6)","traceback":["\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-1ff7646f9e93>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    X_test:, 3:] = sc.transform(X_test[:, 3:]) # The same scaler is applied on the test set, but only the transform method is # used because it must be fitted to the training test features (This is very important)\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"DWPET8ZdlMnu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":154},"outputId":"dea86927-5124-4e2a-e974-2804df9a913c","executionInfo":{"status":"ok","timestamp":1587622313752,"user_tz":-240,"elapsed":767,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}}},"source":["print(X_train)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n"," [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n"," [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n"," [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n"," [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n"," [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n"," [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n"," [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sTXykB_QlRjE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"b68f0cfc-d07c-48cb-80d0-6800028c41f9","executionInfo":{"status":"ok","timestamp":1587622315942,"user_tz":-240,"elapsed":506,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}}},"source":["print(X_test)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n"," [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"],"name":"stdout"}]}]}